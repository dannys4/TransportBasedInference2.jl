# 3. Structure discovery for the Lorenz-96 problem

In this notebook, we will show how we can estimate the conditional independence structure of a random variable $\mathsf{X} \in \mathbb{R}^{N_x}$ with density $\pi$ based on i.i.d. samples $\{\boldsymbol{x}^i\}, \; i = 1, \ldots, N_e$ from $\pi$.

```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
using Revise
using LinearAlgebra
using TransportBasedInference
using Statistics
using Distributions
using OrdinaryDiffEq
using ProgressMeter
```

Load some packages to make nice figures


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
using Plots
default(tickfont = font("CMU Serif", 9),
        titlefont = font("CMU Serif", 14),
        guidefont = font("CMU Serif", 12),
        legendfont = font("CMU Serif", 10),
        grid = false)

using LaTeXStrings
using ColorSchemes
```

The Lorenz-96 model [^1] is a famous problem used in data assimilation and weather prediction. It was derived from first principles as a one-dimensional model for the response of the mid-latitude atmosphere to forcing input. For certain forcing input, it can exhibit a chaotic behavior: sensitivity to initial conditions, strong mixing. In this notebook, we consider the fifteen-dimensional version of the Lorenz-96 model.  The state $\boldsymbol{x} = (x_1, \ldots, x_{15})$ at time $t$ is governed by the following set of ordinary differential equations:

$$\frac{\mathrm{d}x_i}{\mathrm{d}t} = (x_{i+1} - x_{i-2}) x_{i-1} -x_i + F,$$
where it is assumed that $x_{-1} = x_{n − 1} , x_0 = x_{n}$ and $x_{n+1} = x_1$, setting the forcing input $F=8.0$ leads to chaos.

We integrate the Lorenz-96 problem with our favorite ODE library: OrdinaryDiffEq.jl.

```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
# Define the distribution for initial condition
Nx = 20
# Number of runs
Ne = 500
πx0 = MvNormal(zeros(Nx), ones(Nx))
# Zero vector of the size of the state
x0 = rand(πx0)

# Time step
tspan = (0.0, 500.0)

# Define the ODE problem
prob = ODEProblem(TransportBasedInference.lorenz96!,x0, tspan)
nothing #hide
```


Create cache to store the different final solutions


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
X = zeros(Nx, Ne)
nothing #hide
```

Solve Lorenz-96 model for the different initial conditions and store the solutions as the columns of `X`


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
@inbounds for i=1:Ne
    probi = remake(prob; u0 = rand(πx0))
    sol = solve(probi, Tsit5(), dt = 0.01, adaptive = false, dense = false, saveat = tspan[end], save_start = false)
    X[:,i] = deepcopy(sol.u[1])
end
nothing #hide
```

We can use the adaptive transport map procedure to estimate the underlying distribution for the state $\mathsf{X}$.


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
m = 60
S = HermiteMap(m, X; diag = true)
nothing #hide
```


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
optimize(S, X, "kfold"; withqr = true, verbose = false, hessprecond = true)
```



```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
plot(S)
```


Baptista et al. [^3] showed that the conditional independence properties of a random variable $\mathsf{X} \in \mathbb{R}^n$ with density $\pi$ are specified by the *Hessian score matrix* $\boldsymbol{\Omega} \in \mathbb{R}^{n \times n}$ defined as:

$$\Omega_{i,j} = \mathrm{E}_{\pi} \left[ |\partial_i\partial_j \log \pi(\boldsymbol{x})|^2  \right] = \int |\partial_i\partial_j \log \pi(\boldsymbol{x})|^2 \pi(\boldsymbol{x}) \; \mathrm{d}\boldsymbol{x}.$$

The pullback density $\boldsymbol{S}^{\sharp} \rho$ can be used to approximate the true density $\pi$, where $\rho$ denotes the standard Gaussian distribution of $\mathbb{R}^n$. Thus, the score matrix is estimated by [^3]:
$$\hat{\Omega}_{i,j} = \mathrm{E}_{\pi} \left[ |\partial_i\partial_j \log \boldsymbol{S}^{\sharp} \rho|^2  \right]$$

In practice, we use a sampled-based approximation of $\hat{\boldsymbol{\Omega}}$ using samples $\{\boldsymbol{x}^k\}, \; k=1,\ldots, N_e$ of $\pi$ [^3]:


$$\hat{\Omega}_{i,j} \approx \sum_{k=1}^{N_e}|\partial_i\partial_j \log \boldsymbol{S}^{\sharp}\rho(\boldsymbol{x}^k )|^2$$

The function `hess_x_log_pdf!` computes in-place the Hessian of the log pdf of the pullback density $\boldsymbol{S}^{\sharp} \rho$. You can also use `hess_x_log_pdf`.


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
Ωhat = zeros(Nx, Nx)
cache = zeros(1, Nx, Nx)
@inbounds for i=1:Ne
    hess_x_log_pdf!(cache, S, X[:,i:i])
    Ωhat .+= copy(cache[1,:,:]).^2
end
rmul!(Ωhat, 1/Ne)
nothing #hide
```


```@example 3.-Structure-discovery-for-the-Lorenz-96-problem
plt = plot(size = (800, 800))

heatmap!(plt,log10.(Ωhat), ratio = 1, yflip = true,
        colorbar = true, color = :plasma, clim = (1.5, Inf), colorbar_title = L"\log_{10}(\hat{\Omega})",
        xlim = (-Inf, Inf), ylim = (-Inf, Inf),
        xlabel = "Index", ylabel = "Index", background_color_inside = palette(:plasma)[1],
        yticks = (reverse(collect(0:10:Nx))))
plt
```

References:

[^1]: Lorenz, E.N., 1996, September. Predictability: A problem partly solved. In Proc. Seminar on predictability (Vol. 1, No. 1).

[^2]: Baptista, R., Zahm, O., & Marzouk, Y. (2020). An adaptive transport framework for joint and conditional density estimation. arXiv preprint arXiv:2009.10303.

[^3]: Baptista, R., Marzouk, Y., Morrison, R.E. and Zahm, O., 2021. Learning non-Gaussian graphical models via Hessian scores and triangular transport. arXiv preprint arXiv:2101.03093.
